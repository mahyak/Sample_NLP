# -*- coding: utf-8 -*-
"""asssignmnet1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jKxOTM3i2HuQ9hY4byQ7SzcKSuyYM9W-
"""

import nltk
nltk.download()

from nltk.book import *
print(text1)
print(text2)
print(text5)

text1.concordance('monstrous')

text1.similar('monsterous')

text2.common_contexts(['monstrous', 'very'])

text4.dispersion_plot(['citizens', 'democracy', 'freedom', 'duties', 'America'])

!pip search numpy
!pip search matplotlib

print(len(text3))

print(len(set(text3)))

print(sorted(set(text3)))

print(len(text3)/len(set(text3)))

print(text3.count('smote'))

def word_percentage(word, corpus):
  return 100* corpus.count(word)/len(corpus)

def lexical_diversity(corpus):
  return len(corpus)/len(set(corpus))

sent1 = ['call', 'me', 'Ishmael','.']
print(sent1)
print(len(sent1))
print(word_percentage)
print(lexical_diversity(sent1))

print(text4[173])
print(text4.index('awaken'))

saying = ['After', 'all', 'is', 'said', 'and', 'done', 'is', 'more', 'said', 'than', 'done']
tokens = set(saying)
tokens = sorted(tokens)
print(tokens[-2:])

freq1 = FreqDist(text1)
vocabulary1 = freq1.keys()
print(vocabulary1)

freq1.plot(50, cumulative=True)
print(freq1.hapaxes())

fdist2 = FreqDist(text2)
print(fdist2)
vocab2 = fdist2.keys()
print(vocab2)
fdist2.plot(10, cumulative = True)

v = set(text1)
word_long = [w for w in v if len(w)>15]
print(sorted(word_long))

corpus = set(text2)
word_long_5 = [word for word in corpus if len(word)>5]
print(sorted(word_long_5))

text = set(text5)
fDist5 = FreqDist(text5)

vocab_sets = [w for w in text if len(w)>7 and fDist5[w]>7]
print(sorted(vocab_sets))

fdist = FreqDist([len(w) for w in text1])
print(fdist)
print(fdist.keys())
print(fdist.items())
print(fdist.max())
print(fdist.freq(3))