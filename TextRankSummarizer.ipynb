{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextRankSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5zSj8d5v1wDRcxX24a0WR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahyak/Sample_NLP/blob/master/TextRankSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXsqqunfS5Ju",
        "colab_type": "text"
      },
      "source": [
        "TextRank is an extractive and unsupervised text summarization technique. The flow of the TextRank algorithm:\n",
        "\n",
        "\n",
        "*   Combine the articles\n",
        "*   Split the texts to indivisula sentences\n",
        "*   Word embedding\n",
        "*   Calculate the similarities between sentences\n",
        "*   Convert similarity matrix to graph and score them\n",
        "*   Use top ranked sentences for summary\n",
        "\n",
        "Domain: single domain multiple documents summarization task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4qNA1pfWNx4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "076ff059-0ef3-4251-abc9-0f138538129a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcPFwy2gTEpc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5be41ef8-3992-4544-bdb4-17e793504efa"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai7LENAAd8PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfDXHQ7QVLc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/drive/My Drive/Colab Notebooks/tennis_articles_v4.csv')\n",
        "\n",
        "sentences = []\n",
        "\n",
        "for sentence in df['article_text']:\n",
        "  sentences.append(sent_tokenize(sentence))\n",
        "\n",
        "sentences = [y for x in sentences for y in x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk_SKjQwXpYi",
        "colab_type": "text"
      },
      "source": [
        "word embedding: GloVe\n",
        "\n",
        "Wikipedia + Gigaword 5 GloVe vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3WG9izsXtIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "bae7a73b-c872-4162-b764-f3c4eae1489f"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-04 15:48:40--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-04 15:48:40--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-04 15:48:41--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.81MB/s    in 6m 28s  \n",
            "\n",
            "2020-05-04 15:55:09 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y80cAzkRYRXl",
        "colab_type": "text"
      },
      "source": [
        "Text preprocessing:\n",
        "\n",
        "\n",
        "*   Remove punctuations\n",
        "*   Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auYp4RBQYVFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "clean_sentences = [s.lower() for s in clean_sentences]\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(sen):\n",
        "  sentence_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "  return sentence_new\n",
        "\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXLmDTL1hFsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embeddings = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGdfYmnylFKV",
        "colab_type": "text"
      },
      "source": [
        "Create vector for our sentences\n",
        "1- fetch vectors (each of size 100 elements) for the constituent words in a sentence \n",
        "2- take mean/average of those vectors to arrive at a consolidated vector for the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl2-5aCxhb-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_vectors = []\n",
        "\n",
        "for sentence in clean_sentences:\n",
        "  if len(sentence) != 0:\n",
        "    vector = sum([word_embeddings.get(word, np.zeros((100,))) for word in sentence.split()])/(len(sentence.split())+0.001)\n",
        "  else:\n",
        "    vector = np.zeros((100,))\n",
        "\n",
        "  sentences_vectors.append(vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDcNzvL-l3H_",
        "colab_type": "text"
      },
      "source": [
        "Use cosine similarity approach for finding the similarities between sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFlEmXznl_rM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similarity_matrix = np.zeros([len(sentences), len(sentences)])\n",
        "\n",
        "for i in range(len(sentence)):\n",
        "  for j in range(len(sentence)):\n",
        "    if i != j:\n",
        "      similarity_matrix[i][j] = cosine_similarity(sentences_vectors[i].reshape(1,100), sentences_vectors[j].reshape(1,100))[0,0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylFnx0u9oISj",
        "colab_type": "text"
      },
      "source": [
        "Convert similarity matrix to graph\n",
        "\n",
        "*   Node: sentences\n",
        "*   Edge: similarity score between two nodes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZkX8CWGoVjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "scores = nx.pagerank(nx_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5QlN6Kip8vC",
        "colab_type": "text"
      },
      "source": [
        "Sort sentences based on their scores and seperate top 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN29PKJnp72n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1a3af197-963e-4fbc-8507-2465bf90bf52"
      },
      "source": [
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "summary = ''\n",
        "\n",
        "for i in range(10):\n",
        "  summary += \" \"+ranked_sentences[i][1]\n",
        "\n",
        "print(summary)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match. I think just because you're in the same sport doesn't mean that you have to be friends with everyone just because you're categorized, you're a tennis player, so you're going to get along with tennis players. When she said she is not really close to a lot of players, is that something strategic that she is doing? I think everyone just thinks because we're tennis players we should be the greatest of friends. Uhm, I'm not really friendly or close to many players. The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much. But ultimately tennis is just a very small part of what we do. He then dropped his serve to love, and let another match point slip in Medvedev's next service game by netting a backhand. I have not a lot of friends away from the courts.' I think every person has different interests.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}